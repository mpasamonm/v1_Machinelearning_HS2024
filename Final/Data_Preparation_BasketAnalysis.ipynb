{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation; Clean, Preprocess, Normalize, Standardize",
   "id": "881f15694c7c424d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## File: Bewegungen.csv",
   "id": "3eddc29323b260f1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-31T13:11:37.490348Z",
     "start_time": "2024-12-31T13:08:56.025586Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of columns to keep (ran into memory issues...)\n",
    "columns_to_keep = [\n",
    "    \"STATUS\", \"SRC_ITEM\", \"SRC_LOT\", \"SRC_QACODE\",\n",
    "    \"DST_QACODE\", \"SRC_WA\", \"SRC_X\", \"SRC_Y\", \"SRC_Z\", \"OUTNUM\", \"CONQTY\",\n",
    "    \"LISNUM\", \"SUMLIS\", \"TRNDAT\", \"USERID\", # , \"TRNNUM\", \"WORNUM\", \"DST_LOT\", \"DST_WA\", \"LOADDAT\", \"CRTDAT\"\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "#    \"WORNUM\": \"int32\",\n",
    "    \"STATUS\": \"int32\",\n",
    "#    \"MOVTYP\": \"category\",\n",
    "#    \"MOVKEY\": \"category\",\n",
    "    \"SRC_ITEM\": \"category\",\n",
    "    \"SRC_LOT\": \"string\",\n",
    "    \"SRC_QACODE\": \"category\",\n",
    "    #\"DST_LOT\": \"string\",\n",
    "    \"DST_QACODE\": \"category\",\n",
    "    \"SRC_WA\": \"category\",\n",
    "    \"SRC_X\": \"category\",\n",
    "    \"SRC_Y\": \"category\",\n",
    "    \"SRC_Z\": \"category\",\n",
    "#    \"DST_WA\": \"category\",\n",
    "    \"CONQTY\": \"int32\",\n",
    "    \"OUTNUM\": \"int32\", # KEY\n",
    "#    \"RELNUM\": \"int32\",\n",
    "    \"LISNUM\": \"category\",\n",
    "    \"SUMLIS\": \"category\",\n",
    "#    \"TRNNUM\": \"int32\",\n",
    "    \"USERID\": \"category\",\n",
    "}\n",
    "\n",
    "# Load the CSV with optimized settings, and only load necessary cols\n",
    "df_bewegungen = pd.read_csv(\n",
    "    '../Data/bewegungen.csv',\n",
    "    usecols=columns_to_keep,  # Only load the specified columns\n",
    "    dtype=dtypes, # use optimized, manually set data types\n",
    "    parse_dates=[\"TRNDAT\"], # have pandas parse the date\n",
    "    low_memory=False # I set this initially to False to have more accurate dtype assignments, can be set to low_memory to improve performance\n",
    ")\n",
    "\n",
    "####\n",
    "# Data Cleaning\n",
    "####\n",
    "\n",
    "# Temporarily convert 'SRC_LOT' to string type for cleaning - (Column \"Artikelcharge\", this LOT usually is a 3 digit int. An Article can have multiple LOTs. I simplify by removing leading zeros and clean up the column from wrong manual usererrors.\n",
    "df_bewegungen['SRC_LOT'] = df_bewegungen['SRC_LOT'].astype(str)  # Convert Categorical to string\n",
    "# Clean and validate the SRC_LOT column\n",
    "def clean_lot(value):\n",
    "    value = value.lstrip('0') if value != '000' else value  # Remove leading zeros unless '000'\n",
    "    if value.isdigit() and 1 <= len(value) <= 3:           # Validate length (1 to 3 digits)\n",
    "        return value.zfill(3)                              # Pad with leading zeros to ensure 3 digits\n",
    "    return None                                            # Remove invalid entries\n",
    "\n",
    "df_bewegungen['SRC_LOT'] = df_bewegungen['SRC_LOT'].apply(clean_lot)\n",
    "# Convert back to Categorical\n",
    "df_bewegungen['SRC_LOT'] = pd.Categorical(df_bewegungen['SRC_LOT'])\n",
    "\n",
    "# Filter STATUS column (10= Offen, 50= Bestätigt, 95= Storniert -- for my purposes I'm only interested in \"Bestätigt\" rows.\n",
    "# Filter the DataFrame to keep only rows with status == 50 (abgeschlossen)\n",
    "df_bewegungen = df_bewegungen[df_bewegungen['STATUS'] == 50]\n",
    "\n",
    "# Filter the DataFrame in place to include only rows where 'SRC_WA' contains \"WA\", excluding all 'WE', 'WA', 'AU' and 'UM' areas\n",
    "df_bewegungen = df_bewegungen[df_bewegungen['SRC_WA'] == 'EG']\n",
    "\n",
    "# Remove rows where USER == 'LXONE'\n",
    "df_bewegungen = df_bewegungen[df_bewegungen['USERID'] != 'LXONE']\n",
    "\n",
    "# Only keep rows with QACODE == 'H'\n",
    "df_bewegungen = df_bewegungen[df_bewegungen['SRC_QACODE'] == 'H']\n",
    "\n",
    "# Remove all additional Crossdocking movements, easiest to pinpoint via SRC_LOT = 000\n",
    "df_bewegungen = df_bewegungen[df_bewegungen['SRC_LOT'] != '000']\n",
    "\n",
    "\n",
    "# Combine SRC_X, SRC_Y, and SRC_Z into a new column STOR_LOC\n",
    "df_bewegungen['STOR_LOC'] = df_bewegungen['SRC_X'].astype(str) + \\\n",
    "                            df_bewegungen['SRC_Y'].astype(str) + \\\n",
    "                            df_bewegungen['SRC_Z'].astype(str)\n",
    "\n",
    "# Change datatypes as needed\n",
    "df_bewegungen = df_bewegungen.astype({\n",
    "#    \"SRC_LOT\": \"category\",\n",
    "#    \"DST_LOT\": \"category\",\n",
    "    \"STOR_LOC\": \"category\",\n",
    "})\n",
    "\n",
    "# Drop the original SRC_X, SRC_Y, and SRC_Z columns\n",
    "df_bewegungen.drop(['SRC_X', 'SRC_Y', 'SRC_Z'], axis=1, inplace=True)\n",
    "\n",
    "# Drop rows with missing values from the DataFrame - dropna (by default, without parameters) removes entire rows which have a NaN or null value\n",
    "df_bewegungen.dropna(inplace=True)\n",
    "\n",
    "# Drop both 'SRC_WA' and 'DST_WA' columns from the DataFrame now that we filtered the data\n",
    "df_bewegungen.drop(columns=['SRC_LOT', 'STATUS', 'SRC_WA', 'SRC_QACODE', 'DST_QACODE'], inplace=True)\n",
    "\n",
    "# Remove unused categories (i.e. Artifacts) across all categorical columns (after all cleaning steps there were pandas retained all defined categories, even though they no longer existed in the data. Not sure if this would've led to problems down the road during embedding/labeling, so I removed them to avoid eventual issues)\n",
    "for col in df_bewegungen.select_dtypes(include='category').columns:\n",
    "    df_bewegungen[col] = df_bewegungen[col].cat.remove_unused_categories()\n",
    "\n",
    "# Verify the changes\n",
    "print(f\"Updated DataFrame shape: {df_bewegungen.shape}\")\n",
    "# :::"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame shape: (17914909, 8)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:44:55.004570Z",
     "start_time": "2024-12-31T13:44:54.922432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_bewegungen.info()\n",
    "df_bewegungen.head()\n",
    "print(f\"DataFrame shape: {df_bewegungen.shape}\")"
   ],
   "id": "bf2de5981340967e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17914909 entries, 0 to 20094144\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   SRC_ITEM  category      \n",
      " 1   CONQTY    int32         \n",
      " 2   OUTNUM    int32         \n",
      " 3   LISNUM    category      \n",
      " 4   SUMLIS    category      \n",
      " 5   TRNDAT    datetime64[ns]\n",
      " 6   USERID    category      \n",
      " 7   STOR_LOC  category      \n",
      "dtypes: category(5), datetime64[ns](1), int32(2)\n",
      "memory usage: 818.0 MB\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m df_bewegungen\u001B[38;5;241m.\u001B[39minfo()\n\u001B[0;32m      2\u001B[0m df_bewegungen\u001B[38;5;241m.\u001B[39mhead()\n\u001B[1;32m----> 3\u001B[0m \u001B[43mdf_bewegungen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As I cannot provide the full file:\n",
    "OUTPUT: DataFrame shape: (17914909, 8) -- 818.0 MB"
   ],
   "id": "6ba8273092b81ebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Filter the Data to only contain Transactiondates > 2024-11-20 -- base is for proof of concept, otherwise - due to extreme cardinality - the computation times are too long.",
   "id": "fccbcfc478a059ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:23:02.440496Z",
     "start_time": "2024-12-31T13:23:02.384319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Define the fixed date for filtering 2024-11-20\n",
    "start_date = pd.Timestamp('2024-11-20')\n",
    "\n",
    "# Filter the DataFrame to include only rows with TRNDAT newer than 01.01.2023\n",
    "df_bewegungen_reduc = df_bewegungen[df_bewegungen['TRNDAT'] > start_date]\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Shape of DataFrame for data newer than 2024-11-20: {df_bewegungen_reduc.shape}\")"
   ],
   "id": "41605f9a1d352486",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame for data newer than 2024-11-20: (61014, 8)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As I cannot provide the full file:\n",
    "OUTPUT: Shape of DataFrame for data newer than 2024-11-20: (61014, 8) -- 102.9 MB"
   ],
   "id": "ed47b712ec3b7f74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:45:08.286068Z",
     "start_time": "2024-12-31T13:45:08.278667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_bewegungen_reduc.info()\n",
    "df_bewegungen_reduc.head()\n",
    "print(f\"DataFrame shape: {df_bewegungen_reduc.shape}\")"
   ],
   "id": "1617befe2f3b5e9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 61014 entries, 19728799 to 20074575\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   SRC_ITEM  61014 non-null  category      \n",
      " 1   CONQTY    61014 non-null  int32         \n",
      " 2   OUTNUM    61014 non-null  int32         \n",
      " 3   LISNUM    61014 non-null  category      \n",
      " 4   SUMLIS    61014 non-null  category      \n",
      " 5   TRNDAT    61014 non-null  datetime64[ns]\n",
      " 6   USERID    61014 non-null  category      \n",
      " 7   STOR_LOC  61014 non-null  category      \n",
      "dtypes: category(5), datetime64[ns](1), int32(2)\n",
      "memory usage: 102.9 MB\n",
      "Updated DataFrame shape: (61014, 8)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## File: wa_kopf.csv",
   "id": "ff55e949fd587e41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:28:24.200661Z",
     "start_time": "2024-12-31T13:27:39.572861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of columns to keep (ran into memory issues...)\n",
    "columns_to_keep = [\n",
    "    \"OUTNUM\", \"DOCNUM\", \"STATUS\", \"PICCOD\", \"CUSNUM\",\n",
    "    \"SHPTYP\", \"TOUR\", # \"TRNDAT\" \"CRTDAT\", \"ORDDAT\", \"DLVDAT\", not needed rn\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "    \"OUTNUM\": \"int32\", # KEY\n",
    "    \"DOCNUM\": \"category\", # Warenausgangsnummer\n",
    "    # \"ORDNUM\": \"category\", # dont think i need this, achieves the same as DOCNUM\n",
    "    \"STATUS\": \"int32\",\n",
    "    \"PICCOD\": \"category\",\n",
    "    \"CUSNUM\": \"category\", # kunde\n",
    "    \"SHPTYP\": \"category\", # versandart\n",
    "    \"TOUR\": \"category\",\n",
    "}\n",
    "\n",
    "df_wa_kopf = pd.read_csv(\n",
    "    '../Data/wa_kopf.csv',\n",
    "    usecols=columns_to_keep,  # Only load the specified columns\n",
    "    dtype=dtypes,             # Use optimized data types\n",
    "    #usecols=columns_to_use,  # Only load required columns\n",
    "    # parse_dates=[\"TRNDAT\"], # \"ORDDAT\", \"DLVDAT\", \"CRTDAT\",  not needed rn\n",
    "    low_memory=False\n",
    ")\n",
    "# Filter the DataFrame to keep only rows with status == 90 (abgeschlossene)\n",
    "df_wa_kopf = df_wa_kopf[df_wa_kopf['STATUS'] == 90]\n",
    "\n",
    "# Drop rows with missing values from the DataFrame - some early data and tests lead to PICCOD, SHPTYP and TOUR being empty (~86 rows)\n",
    "df_wa_kopf.dropna(inplace=True) # inplace=True modifies DataFrame directly without having to create a new one\n",
    "\n",
    "# Drop 'STATUS' column from the DataFrame now that we filtered the data\n",
    "df_wa_kopf.drop(columns=['STATUS'], inplace=True)"
   ],
   "id": "3b7c0d36f8d0b448",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OUTPUT: DateFrame Shape: (19314904, 6) -- 581.1 MB",
   "id": "b123672bc65223ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:45:20.658619Z",
     "start_time": "2024-12-31T13:45:20.653575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_wa_kopf.info()\n",
    "df_wa_kopf.head()\n",
    "print(f\"DataFrame shape: {df_wa_kopf.shape}\")"
   ],
   "id": "2caef0972a74e76b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19314904 entries, 0 to 19377912\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype   \n",
      "---  ------  -----   \n",
      " 0   OUTNUM  int32   \n",
      " 1   DOCNUM  category\n",
      " 2   PICCOD  category\n",
      " 3   CUSNUM  category\n",
      " 4   SHPTYP  category\n",
      " 5   TOUR    category\n",
      "dtypes: category(5), int32(1)\n",
      "memory usage: 581.1 MB\n",
      "Updated DataFrame shape: (19314904, 6)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## File: wa_positionen",
   "id": "bf5504acd137d95b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "! File currently not in use due to memory constraints and no use-case that can use the SHPQTY, RELQTY and RELQTY values yet",
   "id": "9cdd5cdc26ac994e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T14:38:14.814844Z",
     "start_time": "2024-12-31T14:37:45.667013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of columns to keep (ran into memory issues...)\n",
    "columns_to_keep = [\n",
    "    \"OUTNUM\", \"STATUS\", # \"LOT\",\n",
    "    \"ORDQTY\", \"CONQTY\", \"USERID\", \"TRNNUM\" #  \"TRNDAT\", \"CRTDAT\", not needed rn\n",
    "]\n",
    "\n",
    "dtypes = {\n",
    "    \"OUTNUM\": \"int32\",\n",
    "    \"STATUS\": \"int32\",\n",
    "#    \"LOT\": \"string\", # 3 character string, e.G: 001, 002, 006, 012 etc.\n",
    "    \"ORDQTY\": \"int32\",\n",
    "#    \"RELQTY\": \"int32\" # freigegebene Menge\n",
    "#    \"FNDQTY\": \"int32\" # reservierte Menge\n",
    "#    \"SHPQTY\": \"int32\" # versendete Menge\n",
    "    \"CONQTY\": \"float32\", # setting to float for initial load, need to clean the dataframe and change it to int later - otherwise runs into error because of existing float values\n",
    "    \"USERID\": \"category\",\n",
    "    \"TRNNUM\": \"int32\"\n",
    "}\n",
    "\n",
    "# Load the CSV with optimized settings, and only load necessary cols\n",
    "df_wa_positionen = pd.read_csv(\n",
    "    '../Data/wa_positionen.csv',\n",
    "    usecols=columns_to_keep,  # Only load the specified columns\n",
    "    dtype=dtypes,\n",
    "    #usecols=columns_to_use,  # Only load required columns\n",
    "    #dtype=dtypes,            # Use optimized data types\n",
    "    # parse_dates=[\"TRNDAT\",\"CRTDAT\"] # , not needed rn\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Drop rows with missing values from the DataFrame\n",
    "df_wa_positionen.dropna(inplace=True)\n",
    "\n",
    "# Drop rows where CONQTY has a fractional part (i.e., a value with anything after the decimal point)\n",
    "df_wa_positionen = df_wa_positionen[df_wa_positionen[\"CONQTY\"] % 1 == 0]\n",
    "\n",
    "# Convert CONQTY column to integer type to reflect that it no longer has fractions\n",
    "df_wa_positionen[\"CONQTY\"] = df_wa_positionen[\"CONQTY\"].astype(int)"
   ],
   "id": "20ad2f2cd427a523",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataframe shape: (19375223, 7) 741.3 MB",
   "id": "52b8d6869a110e65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T14:38:19.387762Z",
     "start_time": "2024-12-31T14:38:19.099067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_wa_positionen.info()\n",
    "df_wa_positionen.head()\n",
    "print(f\"DataFrame shape: {df_wa_positionen.shape}\")"
   ],
   "id": "34e322adccc6ce07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19375223 entries, 0 to 19375223\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Dtype   \n",
      "---  ------  -----   \n",
      " 0   OUTNUM  int32   \n",
      " 1   STATUS  int32   \n",
      " 2   ITEM    category\n",
      " 3   ORDQTY  int32   \n",
      " 4   CONQTY  int64   \n",
      " 5   USERID  category\n",
      " 6   TRNNUM  int32   \n",
      "dtypes: category(2), int32(4), int64(1)\n",
      "memory usage: 741.3 MB\n",
      "Updated DataFrame shape: (19375223, 7)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Join Tables",
   "id": "84bb0ea2e0c3f223"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For demonstration im only joining the reduced dataframe \"df_bewegungen_reduc\" onto the dataframe df_wa_kopf (without df_wa_positionen) to get a small joined dataframe that I can use as basis for proof of concept. (Considering using the entire Dataset would never be able to compute in a timely manner (years :)) on my Machine with no GPU-Acceleration.)",
   "id": "113ae134262fa109"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:51:30.964035Z",
     "start_time": "2024-12-31T13:51:29.975329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_full = (\n",
    "    pd.merge(df_bewegungen_reduc, df_wa_kopf, on='OUTNUM', how='inner')\n",
    ")\n",
    "\n",
    "df_full.drop(columns=['OUTNUM'], inplace=True)"
   ],
   "id": "cc1f5cbb13425d92",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "shape: (60758, 12) - 278.7 MB",
   "id": "3fe622ca45cefb44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T13:51:54.962202Z",
     "start_time": "2024-12-31T13:51:54.953533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(df_full.info())\n",
    "df_full.head()\n",
    "print(f\"DataFrame shape: {df_full.shape}\")"
   ],
   "id": "e50e21e0dcee7656",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60758 entries, 0 to 60757\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   SRC_ITEM  60758 non-null  category      \n",
      " 1   CONQTY    60758 non-null  int32         \n",
      " 2   LISNUM    60758 non-null  category      \n",
      " 3   SUMLIS    60758 non-null  category      \n",
      " 4   TRNDAT    60758 non-null  datetime64[ns]\n",
      " 5   USERID    60758 non-null  category      \n",
      " 6   STOR_LOC  60758 non-null  category      \n",
      " 7   DOCNUM    60758 non-null  category      \n",
      " 8   PICCOD    60758 non-null  category      \n",
      " 9   CUSNUM    60758 non-null  category      \n",
      " 10  SHPTYP    60758 non-null  category      \n",
      " 11  TOUR      60758 non-null  category      \n",
      "dtypes: category(10), datetime64[ns](1), int32(1)\n",
      "memory usage: 278.7 MB\n",
      "Updated DataFrame shape: (60758, 12)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This Dataframe \"df_full\" shall be the basis for a \"Market Basket Analysis (MBA)\" -> I'm writing the DF into a .parquet file, which I will provide in my linked Onedrive Folder. (28 MB)",
   "id": "38efd52982af2394"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Market Basket Analysis",
   "id": "b0d87a3d21319eeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T14:06:00.326740Z",
     "start_time": "2024-12-31T14:05:58.123761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "# Extract the relevant columns into a new DataFrame\n",
    "df_mba = df_full[['SRC_ITEM', 'DOCNUM']]\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "df_mba.to_parquet('df_mba.parquet', index=False)\n",
    "\n",
    "print(\"Data extracted and saved to df_mba.parquet successfully.\")"
   ],
   "id": "26aa8f3084443138",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to df_mba.parquet successfully.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The following code find frequent itemsets and generates association rules from raw transactional data.\n",
    "\n",
    "-Loads and prunes data (rare items, empty transactions) for efficiency\n",
    "-one-hot encodes transactions\n",
    "-uses FP Growth algorithm to find frequent itemsets.\n",
    "-then uses frequent itemsets to generate association rules"
   ],
   "id": "340a9fcae13785fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-31T14:28:59.061820Z",
     "start_time": "2024-12-31T14:25:35.733902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Load the parquet file\n",
    "df = pd.read_parquet(\"df_mba.parquet\", columns=[\"SRC_ITEM\", \"DOCNUM\"])\n",
    "\n",
    "# Optional Pruning: Keep items with minimum support -- considering computational limitations I had to play around with these values to receive an Output\n",
    "min_item_support = 20  # Adjust threshold based on data size\n",
    "item_counts = df[\"SRC_ITEM\"].value_counts()\n",
    "frequent_items = item_counts[item_counts >= min_item_support].index\n",
    "df = df[df[\"SRC_ITEM\"].isin(frequent_items)]\n",
    "\n",
    "# Remove unused categories\n",
    "df[\"SRC_ITEM\"] = df[\"SRC_ITEM\"].astype('category')\n",
    "df[\"SRC_ITEM\"] = df[\"SRC_ITEM\"].cat.remove_unused_categories()\n",
    "\n",
    "# Group transactions (explicitly set observed=False to silence warnings) - produces a list of items for each DOCNUM\n",
    "transactions = df.groupby(\"DOCNUM\", observed=False)[\"SRC_ITEM\"].apply(list)\n",
    "transactions = transactions[transactions.apply(len) > 0]  # Remove empty transactions\n",
    "print(f\"Number of transactions after filtering empty ones: {len(transactions)}\")\n",
    "print(transactions.head()) # debugging purposes to see how transactions are stored\n",
    "\n",
    "# Encode transactions using `sparse=True` - transforms data into a one-hot encoded (sparse) matrix format for memory efficiency\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions, sparse=True)\n",
    "\n",
    "# Convert the sparse matrix to a sparse DataFrame with explicit SparseDtype\n",
    "df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_).astype(pd.SparseDtype(\"bool\", fill_value=False))\n",
    "\n",
    "print(f\"Encoded DataFrame shape: {df_encoded.shape}\")\n",
    "# Apply FP-Growth\n",
    "min_support = 0.001  # Lower support threshold - i.e. with 100k entries, a support threshold of 0.01 means 1% of all transactions\n",
    "frequent_itemsets = fpgrowth(df_encoded, min_support=min_support, use_colnames=True)\n",
    "\n",
    "# Filter itemsets with more than one item\n",
    "#frequent_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "# Check if itemsets are found\n",
    "if frequent_itemsets.empty:\n",
    "    print(\"No frequent itemsets found. reduce `min_support` and adjust pruning.\")\n",
    "else:\n",
    "    # Add the `num_itemsets` column required for association_rules\n",
    "    # Calculate number of transactions\n",
    "    num_itemsets = len(transactions)  # Total number of transactions\n",
    "\n",
    "    print(\"Frequent Itemsets:\")\n",
    "    print(frequent_itemsets)\n",
    "\n",
    "    # Generate association rules - num_itemsets should be size of transactions to do correct statistical calculations\n",
    "    # lift = confidence/expected confidence if items independent, increasing min_threshold will lead to fewer rules -> strongest correlations only\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0, num_itemsets=num_itemsets)\n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    print(rules)\n"
   ],
   "id": "22b53fa6922c8886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions after filtering empty ones: 5008\n",
      "DOCNUM\n",
      "02713239e                                            [2311534]\n",
      "02713241e    [46446861, 46446861, 46446861, 46446861, 46446...\n",
      "02713314e                                           [46365590]\n",
      "02714106e                                           [47309357]\n",
      "16579459                                  [46382948, 46382948]\n",
      "Name: SRC_ITEM, dtype: object\n",
      "Encoded DataFrame shape: (5008, 176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manu\\AppData\\Local\\Temp\\ipykernel_25760\\2268555314.py:29: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  df_encoded = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_).astype(pd.SparseDtype(\"bool\", fill_value=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "      support                        itemsets\n",
      "0    0.003994                       (2311534)\n",
      "1    0.002196                      (46446861)\n",
      "2    0.004593                      (46365590)\n",
      "3    0.039337                      (47309357)\n",
      "4    0.026158                      (46382948)\n",
      "..        ...                             ...\n",
      "819  0.001398  (46347654, 46349329, 45508974)\n",
      "820  0.001797            (46365742, 46364898)\n",
      "821  0.002596            (46349329, 46365742)\n",
      "822  0.001997            (47309331, 46365742)\n",
      "823  0.001198  (47309331, 46349329, 46365742)\n",
      "\n",
      "[824 rows x 2 columns]\n",
      "\n",
      "Association Rules:\n",
      "               antecedents           consequents  antecedent support  \\\n",
      "0               (43112467)            (46048383)            0.013379   \n",
      "1               (46048383)            (43112467)            0.013778   \n",
      "2               (46357429)            (46358239)            0.006190   \n",
      "3               (46358239)            (46357429)            0.011182   \n",
      "4               (46357429)            (46358237)            0.006190   \n",
      "...                    ...                   ...                 ...   \n",
      "7285  (47309331, 46365742)            (46349329)            0.001997   \n",
      "7286  (46349329, 46365742)            (47309331)            0.002596   \n",
      "7287            (47309331)  (46349329, 46365742)            0.012979   \n",
      "7288            (46349329)  (47309331, 46365742)            0.020567   \n",
      "7289            (46365742)  (47309331, 46349329)            0.006989   \n",
      "\n",
      "      consequent support   support  confidence       lift  representativity  \\\n",
      "0               0.013778  0.008986    0.671642  48.747567               1.0   \n",
      "1               0.013379  0.008986    0.652174  48.747567               1.0   \n",
      "2               0.011182  0.002396    0.387097  34.617512               1.0   \n",
      "3               0.006190  0.002396    0.214286  34.617512               1.0   \n",
      "4               0.007588  0.001398    0.225806  29.758913               1.0   \n",
      "...                  ...       ...         ...        ...               ...   \n",
      "7285            0.020567  0.001198    0.600000  29.172816               1.0   \n",
      "7286            0.012979  0.001198    0.461538  35.559763               1.0   \n",
      "7287            0.002596  0.001198    0.092308  35.559763               1.0   \n",
      "7288            0.001997  0.001198    0.058252  29.172816               1.0   \n",
      "7289            0.002995  0.001198    0.171429  57.234286               1.0   \n",
      "\n",
      "      leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0     0.008801    3.003494       0.992768  0.494505   0.667054    0.661908  \n",
      "1     0.008801    2.836537       0.993170  0.494505   0.647457    0.661908  \n",
      "2     0.002327    1.613334       0.977162  0.160000   0.380166    0.300691  \n",
      "3     0.002327    1.264849       0.982095  0.160000   0.209392    0.300691  \n",
      "4     0.001351    1.281866       0.972416  0.112903   0.219887    0.205008  \n",
      "...        ...         ...            ...       ...        ...         ...  \n",
      "7285  0.001157    2.448582       0.967654  0.056075   0.591600    0.329126  \n",
      "7286  0.001164    1.833039       0.974408  0.083333   0.454458    0.276923  \n",
      "7287  0.001164    1.098835       0.984658  0.083333   0.089945    0.276923  \n",
      "7288  0.001157    1.059735       0.986001  0.056075   0.056368    0.329126  \n",
      "7289  0.001177    1.203282       0.989443  0.136364   0.168939    0.285714  \n",
      "\n",
      "[7290 rows x 14 columns]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6758b1c5d3a02e25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
